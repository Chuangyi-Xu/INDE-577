{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d357466",
   "metadata": {},
   "source": [
    "## 1.  Random Forest\n",
    "\n",
    "### 1.1 Motivation\n",
    "\n",
    "A single decision tree is easy to interpret and can model non-linear patterns, but it is often **high-variance**: small changes in the training data may lead to very different trees and unstable predictions.  \n",
    "**Random Forest** addresses this by building **many** decision trees and combining their outputs, which typically improves generalization performance.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Core Idea (Ensemble of Trees)\n",
    "\n",
    "Random Forest is an **ensemble method** based on the idea of **bagging (bootstrap aggregating)**:\n",
    "\n",
    "1. Train many decision trees on **different bootstrap samples** of the training set.\n",
    "    \n",
    "2. At each split, each tree considers only a **random subset of features**.\n",
    "    \n",
    "3. Combine all trees:\n",
    "    \n",
    "    - **Classification:** majority vote\n",
    "        \n",
    "    - **Regression:** average prediction\n",
    "        \n",
    "\n",
    "This introduces randomness in both **data** and **features**, which reduces correlation among trees and reduces variance.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.3 Bootstrapping (Sampling with Replacement)\n",
    "\n",
    "Given a training set of size nnn, a bootstrap sample is created by randomly drawing nnn points **with replacement**.  \n",
    "As a result:\n",
    "\n",
    "- some samples appear multiple times\n",
    "    \n",
    "- some samples may not appear at all\n",
    "    \n",
    "\n",
    "Each tree is trained on its own bootstrap sample, so different trees see different training sets.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.4 Feature Subsampling (Random Subspace)\n",
    "\n",
    "When splitting a node, instead of searching over all ddd features, Random Forest only considers mmm randomly selected features:\n",
    "\n",
    "$$\\large m = \\begin{cases} \\sqrt{d}, & \\text{(common for classification)} \\\\ \\log_2(d), & \\text{(another common choice)} \\\\ d, & \\text{(no feature randomness)} \\end{cases}$$\n",
    "\n",
    "This feature randomness further decorrelates trees, making the ensemble more robust.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.5 Prediction Rules\n",
    "\n",
    "Assume we have $\\large B$ trees $\\large \\{T_1, T_2, \\dots, T_B\\}$.\n",
    "#### Regression\n",
    "\n",
    "$$\\large \\hat{y}(x) = \\frac{1}{B}\\sum_{b=1}^{B} T_b(x)$$\n",
    "#### Classification\n",
    "\n",
    "$$\\large \\hat{y}(x) = \\text{mode}\\{T_1(x), T_2(x), \\dots, T_B(x)\\}$$\n",
    "\n",
    "---\n",
    "\n",
    "### 1.6 Why It Works (Bias–Variance Intuition)\n",
    "\n",
    "- A **deep decision tree** typically has **low bias** but **high variance**.\n",
    "    \n",
    "- Random Forest reduces variance by averaging many noisy, high-variance models.\n",
    "    \n",
    "\n",
    "As long as individual trees are not perfectly correlated, averaging them yields a more stable predictor.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.7 Key Hyperparameters\n",
    "\n",
    "- `n_estimators`: number of trees in the forest  \n",
    "    (more trees → more stable, but slower)\n",
    "    \n",
    "- `max_depth`: maximum depth of each tree  \n",
    "    (controls overfitting)\n",
    "    \n",
    "- `min_samples_split`: minimum samples required to split a node  \n",
    "    (regularization)\n",
    "    \n",
    "- `max_features`: number of features considered at each split  \n",
    "    (controls randomness and correlation)\n",
    "    \n",
    "- `bootstrap`: whether to use bootstrap sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8173c0",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preprocessing\n",
    "\n",
    "We use the Seoul Bike Sharing Demand dataset, which contains hourly records of bike rental counts along with weather and temporal features.\n",
    "The task is a regression problem, where the goal is to predict the number of rented bikes in each hour.\n",
    "\n",
    "Each row corresponds to one hour\n",
    "\n",
    "Target variable: Rented Bike Count\n",
    "\n",
    "Features include temperature, humidity, wind speed, and other weather-related variables\n",
    "\n",
    "This dataset exhibits strong non-linear relationships, making it suitable for tree-based and ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26f3d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8760, 14)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"../../data/SeoulBikeData.csv\",\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "df.head()\n",
    "\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad65244b",
   "metadata": {},
   "source": [
    "For this example, we select a subset of numeric features to keep the focus on the learning algorithms rather than extensive feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46b44126",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"Hour\",\n",
    "    \"Temperature(°C)\",\n",
    "    \"Humidity(%)\",\n",
    "    \"Wind speed (m/s)\",\n",
    "    \"Visibility (10m)\",\n",
    "    \"Dew point temperature(°C)\",\n",
    "    \"Solar Radiation (MJ/m2)\",\n",
    "    \"Rainfall(mm)\",\n",
    "    \"Snowfall (cm)\",\n",
    "]\n",
    "\n",
    "target = \"Rented Bike Count\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de1cf93",
   "metadata": {},
   "source": [
    "These features are:\n",
    "\n",
    "- continuous or ordinal,\n",
    "\n",
    "- directly usable by tree-based models,\n",
    "\n",
    "- known to influence bike rental demand.\n",
    "\n",
    "For simplicity, we remove rows with missing values in the selected columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d5c73f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8760, 9), (8760,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df[features + [target]].dropna()\n",
    "\n",
    "X = df_model[features].values\n",
    "y = df_model[target].values\n",
    "\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a007fdd",
   "metadata": {},
   "source": [
    "Tree-based models do not require feature scaling,\n",
    "so no normalization or standardization is applied.\n",
    "\n",
    "We split the dataset into training and testing sets to evaluate model generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ad00b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7008, 9), (1752, 9))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c008178",
   "metadata": {},
   "source": [
    "- 80% of the data is used for training\n",
    "\n",
    "- 20% is held out for testing\n",
    "\n",
    "- A fixed random seed ensures reproducibility\n",
    "\n",
    "We use **Mean Squared Error (MSE)** as the evaluation metric:\n",
    "\n",
    "$$\\large \\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "MSE penalizes large prediction errors and is commonly used for regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7b8ca4",
   "metadata": {},
   "source": [
    "## 3. Decision Tree Baseline\n",
    "\n",
    "Decision trees are flexible and capable of modeling non-linear relationships, but they often suffer from high variance, which makes them a natural point of comparison for ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145dc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rice_ml.decision_trees import DecisionTreeRegressor\n",
    "\n",
    "# Initialize Decision Tree Regressor\n",
    "dt = DecisionTreeRegressor(\n",
    "    max_depth=6,\n",
    "    min_samples_split=10,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fff51b",
   "metadata": {},
   "source": [
    "- max_depth limits tree complexity and helps control overfitting\n",
    "\n",
    "- min_samples_split prevents splits based on very few samples\n",
    "\n",
    "These hyperparameters are chosen to provide a reasonable balance between bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75533aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rice_ml.decision_trees.DecisionTreeRegressor at 0x1dbd0719850>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca5590f",
   "metadata": {},
   "source": [
    "The model is trained using the training portion of the dataset.\n",
    "\n",
    "We evaluate the model on the held-out test set using Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c91d6ca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137778.64701923067"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "\n",
    "# Compute MSE\n",
    "dt_mse = mean_squared_error(y_test, y_pred_dt)\n",
    "dt_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171cfafa",
   "metadata": {},
   "source": [
    "The MSE serves as a quantitative measure of prediction error.\n",
    "\n",
    "Decision trees can fit complex patterns by recursively partitioning the feature space.\n",
    "However, because the model structure depends heavily on the training data, a single tree is often sensitive to noise and may not generalize well to unseen samples.\n",
    "\n",
    "This behavior motivates the use of Random Forest, which reduces variance by aggregating multiple decision trees trained on randomized data subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db164d2d",
   "metadata": {},
   "source": [
    "## 4. Random Forest Regressor\n",
    "\n",
    "Random Forest is an ensemble learning method that builds multiple decision trees and combines their predictions to reduce variance and improve generalization performance.\n",
    "\n",
    "We train a Random Forest Regressor using our own implementation and compare its behavior with the single decision tree baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd1f97fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rice_ml.random_forest import RandomForestRegressor\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=8,\n",
    "    min_samples_split=10,\n",
    "    max_features=\"sqrt\",\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8473379",
   "metadata": {},
   "source": [
    "Hyperparameter explanation:\n",
    "\n",
    "- n_estimators: number of trees in the forest\n",
    "\n",
    "- max_depth: maximum depth of each tree\n",
    "\n",
    "- min_samples_split: minimum samples required to split a node\n",
    "\n",
    "- max_features: number of features randomly selected at each split\n",
    "\n",
    "- random_state: ensures reproducibility\n",
    "\n",
    "These settings introduce randomness while controlling model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6dc7771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rice_ml.random_forest.RandomForestRegressor at 0x1dbd0719d00>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d84143b",
   "metadata": {},
   "source": [
    "Each tree is trained on a bootstrap sample of the training data, and feature randomness is applied at each split.\n",
    "\n",
    "We evaluate the Random Forest model using the same test set and evaluation metric as the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81d51ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111629.14420404252"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test data\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Compute MSE\n",
    "rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
    "rf_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb887978",
   "metadata": {},
   "source": [
    "Using the same metric allows for a fair comparison between models.\n",
    "\n",
    "Compared to a single decision tree, Random Forest produces smoother predictions and is less sensitive to noise in the training data.\n",
    "By averaging multiple de-correlated trees, the ensemble reduces variance while maintaining low bias.\n",
    "\n",
    "This behavior is particularly beneficial for datasets with complex, non-linear relationships, such as bike rental demand influenced by weather conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f330191c",
   "metadata": {},
   "source": [
    "## 5. Model Comparison and Key Takeaways\n",
    "\n",
    "Compare the performance of the Decision Tree Regressor and the Random Forest Regressor using the same training–testing split and evaluation metric.\n",
    "\n",
    "Summarize the test set performance using **Mean Squared Error (MSE)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1021880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>137778.647019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>111629.144204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model       Test MSE\n",
       "0  Decision Tree  137778.647019\n",
       "1  Random Forest  111629.144204"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"Model\": [\"Decision Tree\", \"Random Forest\"],\n",
    "    \"Test MSE\": [dt_mse, rf_mse]\n",
    "})\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ac305",
   "metadata": {},
   "source": [
    "Observe that the Random Forest achieves a **lower MSE** than the single decision tree, indicating improved generalization performance.\n",
    "\n",
    "The performance difference can be explained by the **bias–variance trade-off**:\n",
    "\n",
    "- The decision tree has **low bias but high variance**, making it sensitive to fluctuations in the training data.\n",
    "\n",
    "- The random forest reduces variance by averaging predictions from many de-correlated trees trained on different bootstrap samples.\n",
    "\n",
    "As a result, Random Forest typically provides more stable and reliable predictions on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13749bba",
   "metadata": {},
   "source": [
    "| Aspect              | Decision Tree | Random Forest     |\n",
    "| ------------------- | ------------- | ----------------- |\n",
    "| Model complexity    | Single tree   | Ensemble of trees |\n",
    "| Variance            | High          | Lower             |\n",
    "| Robustness to noise | Low           | High              |\n",
    "| Interpretability    | High          | Lower             |\n",
    "| Generalization      | Moderate      | Strong            |\n",
    "This comparison highlights the trade-off between interpretability and predictive performance.\n",
    "\n",
    "- Random Forest significantly improves predictive performance over a single decision tree on this dataset.\n",
    "    \n",
    "- The improvement comes from **bootstrap aggregation** and **feature randomness**, which reduce model variance.\n",
    "    \n",
    "- Tree-based ensemble methods work well with minimal preprocessing and can capture complex non-linear relationships.\n",
    "    \n",
    "- Random Forest serves as a strong baseline for many real-world regression tasks.\n",
    "\n",
    "This example demonstrates how ensemble learning can enhance model performance by combining multiple weak learners.  \n",
    "Compared to a single decision tree, Random Forest provides more robust predictions and better generalization, making it a powerful tool for practical machine learning applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
